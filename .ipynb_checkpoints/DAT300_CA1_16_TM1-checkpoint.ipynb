{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compulsory assignment 1 - Using Dask-ml on large data\n",
    "## Group #16\n",
    "### Thomas Moen and JÃ¸rgen Kongsro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_ml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bd6249cfa52d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dask_ml'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import os\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API\n",
    "# How to setup: https://github.com/Kaggle/kaggle-api\n",
    "# or visit: https://adityashrm21.github.io/Setting-Up-Kaggle/\n",
    "\n",
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle data using Kaggle API\n",
    "#!kaggle competitions download -c dat300-ca1-autumn-2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Kaggle data\n",
    "#!unzip \"dat300-ca1-autumn-2019.zip\" -d \"/tmp/whatever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose here which imputation methods to use:\n",
    "\n",
    "all_imputation_methods = ['col_means', 'correlated_columns']\n",
    "imputation_methods = ['correlated_columns', 'col_means']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test.csv', 'X_train.csv', 'y_test_sampleSubmission.csv', 'y_train.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List files in directory\n",
    "\n",
    "if os.name == 'nt':\n",
    "    workdir = 'C://Users//thomoe//Documents//myDAT300//dat300-ca1-autumn-2019//'\n",
    "elif os.name == 'posix':\n",
    "    workdir = '/Users/jorgenkongsro/Downloads/dat300-ca1-autumn-2019/'\n",
    "    \n",
    "os.listdir(workdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def import_data(csv_file):\n",
    "    \"\"\" Import data from csv file\n",
    "\n",
    "    :param data: a .csv separated dataset\n",
    "    :return: a pandas data array, df\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df = dd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "x_train_df = import_data(workdir + 'X_train.csv')\n",
    "x_test_df = import_data(workdir + 'X_test.csv')\n",
    "y_train_df = import_data(workdir + 'y_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dimensions etc:\n",
    "\n",
    "x_train_df_nrows = len(x_train_df)\n",
    "x_train_df_ncols = len(x_train_df.columns)\n",
    "x_test_df_nrows = len(x_test_df)\n",
    "x_test_df_ncols = len(x_test_df.columns)\n",
    "\n",
    "x_train_df_colnames = list(x_train_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 31.1s\n",
      "f1      3.628002\n",
      "f2      3.754723\n",
      "f3      3.754723\n",
      "f4      4.133142\n",
      "f5      4.133142\n",
      "f6      4.133142\n",
      "f7      9.171993\n",
      "f8      9.276405\n",
      "f9      9.410288\n",
      "f10     3.628002\n",
      "f11     3.754723\n",
      "f12     3.836795\n",
      "f13     4.133142\n",
      "f14     4.299061\n",
      "f15     4.377457\n",
      "f16     9.171993\n",
      "f17     9.276405\n",
      "f18     9.410288\n",
      "f19     3.628002\n",
      "f20     3.754723\n",
      "f21     3.836795\n",
      "f22     4.133142\n",
      "f23     4.299061\n",
      "f24     4.377457\n",
      "f25     9.171993\n",
      "f26     9.276405\n",
      "f27     9.410288\n",
      "f28     3.628002\n",
      "f29     3.754723\n",
      "f30     3.836795\n",
      "          ...   \n",
      "f133    9.171993\n",
      "f134    9.276405\n",
      "f135    9.410288\n",
      "f136    3.628002\n",
      "f137    3.754723\n",
      "f138    3.836795\n",
      "f139    4.133142\n",
      "f140    4.299061\n",
      "f141    4.377457\n",
      "f142    9.171993\n",
      "f143    9.276405\n",
      "f144    9.410288\n",
      "f145    3.628002\n",
      "f146    3.754723\n",
      "f147    3.836795\n",
      "f148    4.133142\n",
      "f149    4.299061\n",
      "f150    4.377457\n",
      "f151    9.171993\n",
      "f152    9.276405\n",
      "f153    9.410288\n",
      "f154    3.628002\n",
      "f155    3.754723\n",
      "f156    3.836795\n",
      "f157    4.133142\n",
      "f158    4.299061\n",
      "f159    4.377457\n",
      "f160    9.171993\n",
      "f161    9.276405\n",
      "f162    9.410288\n",
      "Length: 162, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnote: the results indicate that the features come in \"tripets\", e.g. f1 to f3 have quite similar missing%. \\nWe could impute some values very precisely by insert the mean of the other values within the triplet, \\nif only one or two values are missing\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def percent_missing(dataframe):\n",
    "    \"\"\" Check for percent missing values in dataframe\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    missing_values = dataframe.isnull().sum()\n",
    "    \n",
    "    with ProgressBar():\n",
    "        percent_missing = ((missing_values / dataframe.index.size) * 100).compute()\n",
    "        \n",
    "    return percent_missing\n",
    "\n",
    "\n",
    "print(percent_missing(x_train_df))\n",
    "\n",
    "\"\"\"\n",
    "note: the results indicate that the features come in \"tripets\", e.g. f1 to f3 have quite similar missing%. \n",
    "We could impute some values very precisely by insert the mean of the other values within the triplet, \n",
    "if only one or two values are missing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dropna() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fd3e34668cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnan_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mpercent_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-fd3e34668cb7>\u001b[0m in \u001b[0;36mnan_filter\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dropna() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "def nan_filter(dataframe):\n",
    "    \"\"\" Filter out NaN\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    filtered = dataframe.dropna(how='all', axis= 0, subset=None, thresh=None).compute()\n",
    "\n",
    "    return filtered\n",
    "\n",
    "filtered = nan_filter(x_train_df)\n",
    "percent_missing(filtered)\n",
    "\n",
    "#THIS ONE DOES NOT WORK, THE AXIS PARAMETER SEEMS TO HAVE BEEN DEPRECATED. \n",
    "#DROPPING A WITHIN SPECIFYING ROWS OR COLUMNS DID NOT REMOVE ANY OF EITHER\n",
    "\n",
    "#remove columns where all observations are NaN: \n",
    "#print(len(x_train_df.columns))\n",
    "#print(len(x_train_df))\n",
    "#x_train_df_nafiltered = x_train_df.dropna(how='all').compute()\n",
    "#print(len(x_train_df.columns))\n",
    "#print(len(x_train_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "[]\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "def drop_columns(df, thresh_perc, ncols = None, nrows = None):\n",
    "    \"\"\" Drop columns which have more than 50% missing\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if ncols == None: ncols = len(df.columns)\n",
    "    if nrows == None: nrows = len(df)\n",
    "    \n",
    "    nmissing = list(df.isna().sum().compute())\n",
    "    colstodrop = [a for a in range(ncols) if nmissing[a]/nrows*100 >= thresh_perc]\n",
    "    colstodrop = [df.columns[a] for a in colstodrop]\n",
    "    return df.drop(colstodrop, axis='columns')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No features had more than 50 percent missing observations\n"
     ]
    }
   ],
   "source": [
    "#delete columns (features) with more than N % missing observations from training data set: \n",
    "missing_perc = 50\n",
    "x_train_df = drop_columns(x_train_df, missing_perc, x_train_df_ncols, x_train_df_nrows)\n",
    "\n",
    "diff = x_train_df_ncols - len(x_train_df.columns)\n",
    "if diff > 0: \n",
    "    print('Deleted', diff, 'features because they had more than', missing_perc, 'percent missing observations')\n",
    "    x_train_df_ncols = len(x_train_df.columns)\n",
    "else:\n",
    "    print('No features had more than', missing_perc, 'percent missing observations')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2 f3\n",
      "f3 f2\n"
     ]
    }
   ],
   "source": [
    "#impute by inserting value from most closely correlated column: \n",
    "\n",
    "\"\"\"\n",
    "I can't understand why this one doesn't work. The idea was to fill in NaN's from correlated columns.\n",
    "The problem might be in the last line\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#will only correct if the correlation between columns is above this threshold: \n",
    "lowest_allowed_corr = 0.995\n",
    "\n",
    "if 'correlated_columns' in imputation_methods:\n",
    "    \n",
    "    #if correlation matrix does not exist, first try to read it from file, if that does not work calculate it: \n",
    "    if dir().count('corrs') == 0: \n",
    "        try: \n",
    "            corrs = pd.read_csv(workdir + 'features_correlation_matrix.txt')\n",
    "        except:\n",
    "            corrs = x_train_df.corr('pearson')\n",
    "    \n",
    "    #impute for each feature feat:\n",
    "    for feat in x_train_df_colnames:\n",
    "        \n",
    "        #order the feature names according to (absolute value of) correlations to feat:\n",
    "        abscorr = [abs(a) for a in list(corrs[feat])]\n",
    "        order = np.argsort(abscorr)[::-1]\n",
    "        topfeatures = [x_train_df_colnames[a] for a in order]\n",
    "        \n",
    "        #remove features which are not sufficiently correlated to feat:\n",
    "        mapper = dict(zip(x_train_df_colnames, abscorr))\n",
    "        topfeatures = [a for a in topfeatures if mapper[a] >= lowest_allowed_corr]\n",
    "        \n",
    "        #correct using each feature in topfeatures, starting with the most strongly correlated otherfeature:\n",
    "        for otherfeat in [a for a in topfeatures if a != feat]:\n",
    "            print(feat, otherfeat)\n",
    "            x_train_df_trim[feat] = x_train_df_trim[feat].fillna(x_train_df_trim[otherfeat]).compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute by inserting the mean of the column in question, for all columns: \n",
    " \n",
    "if 'col_means' in imputation_methods:\n",
    "    \n",
    "    #calculate mean (note that axis needs to be 0 to get columns, which is weird)\n",
    "    miin = x_train_df.mean(axis = 0).compute()\n",
    "\n",
    "    x_train_df_imean = x_train_df.fillna(dict(miin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use correlation between columns in order to impute: \n",
    "\n",
    "corr_thresh = 0.90\n",
    "\n",
    "#for j in range(len(x_train_ncols)):\n",
    "#    col_j = \n",
    "#    for k in range(len(x_train_ncols)) if j != k: \n",
    " \n",
    "#len(x_train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(dataframe):\n",
    "    \"\"\" Drop rows if more than 0 and less than 5% missing \n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    rows_to_drop = list(percent_missing[(percent_missing > 0) & (percent_missing < 5)].index)\n",
    "    data_clean = dataframe.dropna(subset=rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_rows(dataframe,feature):\n",
    "    \"\"\" Impute rows if more than 5% and less than 50% missing\n",
    "    :param data: dataframe, feature\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    value = median[feature]\n",
    "    df_impute[feature] = df[feature].fillna(value)\n",
    "    return df_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of a feature\n",
    "print (x_train_df.count().compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and train\n",
    "\n",
    "# Split data into train and validation\n",
    "\n",
    "# JÃRGEN KAN BEGYNNE BYGGE DETTE\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "bern = BernoulliNB()\n",
    "with ProgressBar():\n",
    "    bern.fit(x_train_df, y_train_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
