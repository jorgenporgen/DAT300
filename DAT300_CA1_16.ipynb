{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compulsory assignment 1 - Using Dask-ml on large data\n",
    "## Group #16\n",
    "### Thomas Moen and JÃ¸rgen Kongsro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import os\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API\n",
    "# How to setup: https://github.com/Kaggle/kaggle-api\n",
    "# or visit: https://adityashrm21.github.io/Setting-Up-Kaggle/\n",
    "\n",
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle data using Kaggle API\n",
    "#!kaggle competitions download -c dat300-ca1-autumn-2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Kaggle data\n",
    "#!unzip \"dat300-ca1-autumn-2019.zip\" -d \"/tmp/whatever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose here which imputation methods to use:\n",
    "\n",
    "all_imputation_methods = ['col_means', 'correlated_columns']\n",
    "imputation_methods = ['correlated_columns', 'col_means']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_sampleSubmission.csv', 'X_train.csv', 'y_train.csv', 'X_test.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List files in directory\n",
    "\n",
    "if os.name == 'nt':\n",
    "    workdir = 'C://Users//thomoe//Documents//myDAT300//dat300-ca1-autumn-2019//'\n",
    "elif os.name == 'posix':\n",
    "    workdir = '/Users/jorgenkongsro/Downloads/dat300-ca1-autumn-2019/'\n",
    "    \n",
    "os.listdir(workdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def import_data(csv_file):\n",
    "    \"\"\" Import data from csv file\n",
    "\n",
    "    :param data: a .csv separated dataset\n",
    "    :return: a pandas data array, df\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df = dd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "x_train_df = import_data(workdir + 'X_train.csv')\n",
    "x_test_df = import_data(workdir + 'X_test.csv')\n",
    "y_train_df = import_data(workdir + 'y_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dimensions etc:\n",
    "\n",
    "x_train_df_nrows = len(x_train_df)\n",
    "x_train_df_ncols = len(x_train_df.columns)\n",
    "x_test_df_nrows = len(x_test_df)\n",
    "x_test_df_ncols = len(x_test_df.columns)\n",
    "\n",
    "x_train_df_colnames = list(x_train_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 34.9s\n",
      "f1      3.628002\n",
      "f2      3.754723\n",
      "f3      3.836795\n",
      "f4      4.133142\n",
      "f5      4.299061\n",
      "f6      4.377457\n",
      "f7      9.171993\n",
      "f8      9.276405\n",
      "f9      9.410288\n",
      "f10     3.628002\n",
      "f11     3.754723\n",
      "f12     3.836795\n",
      "f13     4.133142\n",
      "f14     4.299061\n",
      "f15     4.377457\n",
      "f16     9.171993\n",
      "f17     9.276405\n",
      "f18     9.410288\n",
      "f19     3.628002\n",
      "f20     3.754723\n",
      "f21     3.836795\n",
      "f22     4.133142\n",
      "f23     4.299061\n",
      "f24     4.377457\n",
      "f25     9.171993\n",
      "f26     9.276405\n",
      "f27     9.410288\n",
      "f28     3.628002\n",
      "f29     3.754723\n",
      "f30     3.836795\n",
      "          ...   \n",
      "f133    9.171993\n",
      "f134    9.276405\n",
      "f135    9.410288\n",
      "f136    3.628002\n",
      "f137    3.754723\n",
      "f138    3.836795\n",
      "f139    4.133142\n",
      "f140    4.299061\n",
      "f141    4.377457\n",
      "f142    9.171993\n",
      "f143    9.276405\n",
      "f144    9.410288\n",
      "f145    3.628002\n",
      "f146    3.754723\n",
      "f147    3.836795\n",
      "f148    4.133142\n",
      "f149    4.299061\n",
      "f150    4.377457\n",
      "f151    9.171993\n",
      "f152    9.276405\n",
      "f153    9.410288\n",
      "f154    3.628002\n",
      "f155    3.754723\n",
      "f156    3.836795\n",
      "f157    4.133142\n",
      "f158    4.299061\n",
      "f159    4.377457\n",
      "f160    9.171993\n",
      "f161    9.276405\n",
      "f162    9.410288\n",
      "Length: 162, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnote: the results indicate that the features come in \"tripets\", e.g. f1 to f3 have quite similar missing%. \\nWe could impute some values very precisely by insert the mean of the other values within the triplet, \\nif only one or two values are missing\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def percent_missing(dataframe):\n",
    "    \"\"\" Check for percent missing values in dataframe\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    missing_values = dataframe.isnull().sum()\n",
    "    \n",
    "    with ProgressBar():\n",
    "        percent_missing = ((missing_values / dataframe.index.size) * 100).compute()\n",
    "        \n",
    "    return percent_missing\n",
    "\n",
    "\n",
    "print(percent_missing(x_train_df))\n",
    "\n",
    "\"\"\"\n",
    "note: the results indicate that the features come in \"tripets\", e.g. f1 to f3 have quite similar missing%. \n",
    "We could impute some values very precisely by insert the mean of the other values within the triplet, \n",
    "if only one or two values are missing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dropna() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fd3e34668cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnan_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mpercent_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-fd3e34668cb7>\u001b[0m in \u001b[0;36mnan_filter\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dropna() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "def nan_filter(dataframe):\n",
    "    \"\"\" Filter out NaN\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    filtered = dataframe.dropna(how='all', axis= 0, subset=None, thresh=None).compute()\n",
    "\n",
    "    return filtered\n",
    "\n",
    "filtered = nan_filter(x_train_df)\n",
    "percent_missing(filtered)\n",
    "\n",
    "#THIS ONE DOES NOT WORK, THE AXIS PARAMETER SEEMS TO HAVE BEEN DEPRECATED. \n",
    "#DROPPING A WITHIN SPECIFYING ROWS OR COLUMNS DID NOT REMOVE ANY OF EITHER\n",
    "\n",
    "#remove columns where all observations are NaN: \n",
    "#print(len(x_train_df.columns))\n",
    "#print(len(x_train_df))\n",
    "#x_train_df_nafiltered = x_train_df.dropna(how='all').compute()\n",
    "#print(len(x_train_df.columns))\n",
    "#print(len(x_train_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, thresh_perc, ncols = None, nrows = None):\n",
    "    \"\"\" Drop columns which have more than 50% missing\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if ncols == None: ncols = len(df.columns)\n",
    "    if nrows == None: nrows = len(df)\n",
    "    \n",
    "    nmissing = list(df.isna().sum().compute())\n",
    "    colstodrop = [a for a in range(ncols) if nmissing[a]/nrows*100 >= thresh_perc]\n",
    "    colstodrop = [df.columns[a] for a in colstodrop]\n",
    "    return df.drop(colstodrop, axis='columns')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features had more than 50 percent missing observations\n"
     ]
    }
   ],
   "source": [
    "#delete columns (features) with more than N % missing observations from training data set: \n",
    "missing_perc = 50\n",
    "x_train_df = drop_columns(x_train_df, missing_perc, x_train_df_ncols, x_train_df_nrows)\n",
    "\n",
    "diff = x_train_df_ncols - len(x_train_df.columns)\n",
    "if diff > 0: \n",
    "    print('Deleted', diff, 'features because they had more than', missing_perc, 'percent missing observations')\n",
    "    x_train_df_ncols = len(x_train_df.columns)\n",
    "else:\n",
    "    print('No features had more than', missing_perc, 'percent missing observations')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2 f3\n",
      "f3 f2\n"
     ]
    }
   ],
   "source": [
    "#impute by inserting value from most closely correlated column: \n",
    "\n",
    "\"\"\"\n",
    "I can't understand why this one doesn't work. The idea was to fill in NaN's from correlated columns.\n",
    "The problem might be in the last line\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#will only correct if the correlation between columns is above this threshold: \n",
    "lowest_allowed_corr = 0.995\n",
    "\n",
    "if 'correlated_columns' in imputation_methods:\n",
    "    \n",
    "    #if correlation matrix does not exist, first try to read it from file, if that does not work calculate it: \n",
    "    if dir().count('corrs') == 0: \n",
    "        try: \n",
    "            corrs = pd.read_csv(workdir + 'features_correlation_matrix.txt')\n",
    "        except:\n",
    "            corrs = x_train_df.corr('pearson')\n",
    "    \n",
    "    #impute for each feature feat:\n",
    "    for feat in x_train_df_colnames:\n",
    "        \n",
    "        #order the feature names according to (absolute value of) correlations to feat:\n",
    "        abscorr = [abs(a) for a in list(corrs[feat])]\n",
    "        order = np.argsort(abscorr)[::-1]\n",
    "        topfeatures = [x_train_df_colnames[a] for a in order]\n",
    "        \n",
    "        #remove features which are not sufficiently correlated to feat:\n",
    "        mapper = dict(zip(x_train_df_colnames, abscorr))\n",
    "        topfeatures = [a for a in topfeatures if mapper[a] >= lowest_allowed_corr]\n",
    "        \n",
    "        #correct using each feature in topfeatures, starting with the most strongly correlated otherfeature:\n",
    "        for otherfeat in [a for a in topfeatures if a != feat]:\n",
    "            print(feat, otherfeat)\n",
    "            x_train_df_trim[feat] = x_train_df_trim[feat].fillna(x_train_df_trim[otherfeat]).compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Series getitem in only supported for other series objects with matching partition structure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4f5f0d64532d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmiin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx_train_df_imean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m         raise NotImplementedError(\n\u001b[0;32m-> 2673\u001b[0;31m             \u001b[0;34m\"Series getitem in only supported for other series objects \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2674\u001b[0m             \u001b[0;34m\"with matching partition structure\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m         )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Series getitem in only supported for other series objects with matching partition structure"
     ]
    }
   ],
   "source": [
    "#impute by inserting the mean of the column in question, for all columns: \n",
    " \n",
    "if 'col_means' in imputation_methods:\n",
    "    \n",
    "    #calculate mean (note that axis needs to be 0 to get columns, which is weird)\n",
    "    miin = x_train_df.mean(axis = 0).compute()\n",
    "\n",
    "    x_train_df_imean = x_train_df.fillna(dict(miin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(dataframe):\n",
    "    \"\"\" Drop rows if more than 0 and less than 5% missing \n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    rows_to_drop = list(percent_missing[(percent_missing > 0) & (percent_missing < 5)].index)\n",
    "    data_clean = dataframe.dropna(subset=rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_rows(dataframe,feature):\n",
    "    \"\"\" Impute rows if more than 5% and less than 50% missing\n",
    "    :param data: dataframe, feature\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    value = median[feature]\n",
    "    df_impute[feature] = df[feature].fillna(value)\n",
    "    return df_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of a feature\n",
    "print (x_train_df.count().compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and train\n",
    "\n",
    "# Import libraries\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.metrics import accuracy_score\n",
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train_df_imean\n",
    "y = y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched divisions. (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None) != (None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1d2b3f80c5be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask_ml/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, blockwise, *arrays, **options)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mcheck_matching_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblockwise\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mblockwise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask_ml/utils.py\u001b[0m in \u001b[0;36mcheck_matching_blocks\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivisions\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdivisions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 220\u001b[0;31m                     \u001b[0;34m\"Mismatched divisions. {} != {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdivisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 )\n\u001b[1;32m    222\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched divisions. (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None) != (None, None)"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_train_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
