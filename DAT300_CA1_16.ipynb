{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compulsory assignment 1 - Using Dask-ml on large data\n",
    "## Group #16\n",
    "### Thomas Moen and JÃ¸rgen Kongsro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import os\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.model_selection import train_test_split\n",
    "print (dask.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API\n",
    "# How to setup: https://github.com/Kaggle/kaggle-api\n",
    "# or visit: https://adityashrm21.github.io/Setting-Up-Kaggle/\n",
    "\n",
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle data using Kaggle API\n",
    "#!kaggle competitions download -c dat300-ca1-autumn-2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Kaggle data\n",
    "#!unzip \"dat300-ca1-autumn-2019.zip\" -d \"/tmp/whatever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose here which imputation methods to use:\n",
    "\n",
    "all_imputation_methods = ['col_means', 'correlated_columns']\n",
    "imputation_methods = ['correlated_columns', 'col_means']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_sampleSubmission.csv', 'X_train.csv', 'y_train.csv', 'X_test.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List files in directory\n",
    "\n",
    "if os.name == 'nt':\n",
    "    workdir = 'C://Users//thomoe//Documents//myDAT300//dat300-ca1-autumn-2019//'\n",
    "elif os.name == 'posix':\n",
    "    workdir = '/Users/jorgenkongsro/Downloads/dat300-ca1-autumn-2019/'\n",
    "    \n",
    "os.listdir(workdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def import_data(csv_file):\n",
    "    \"\"\" Import data from csv file\n",
    "\n",
    "    :param data: a .csv separated dataset\n",
    "    :return: a pandas data array, df\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df = dd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "x_train_df = import_data(workdir + 'X_train.csv')\n",
    "x_test_df = import_data(workdir + 'X_test.csv')\n",
    "y_train_df = import_data(workdir + 'y_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dimensions etc:\n",
    "\n",
    "x_train_df_nrows = len(x_train_df)\n",
    "x_train_df_ncols = len(x_train_df.columns)\n",
    "x_test_df_nrows = len(x_test_df)\n",
    "x_test_df_ncols = len(x_test_df.columns)\n",
    "\n",
    "x_train_df_colnames = list(x_train_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(dataframe):\n",
    "    \"\"\" Check for percent missing values in dataframe\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    missing_values = dataframe.isnull().sum()\n",
    "    \n",
    "    with ProgressBar():\n",
    "        percent_missing = ((missing_values / dataframe.index.size) * 100).compute()\n",
    "        \n",
    "    return percent_missing\n",
    "\n",
    "\n",
    "print(percent_missing(y_train_df))\n",
    "\n",
    "\"\"\"\n",
    "note: the results indicate that the features come in \"tripets\", e.g. f1 to f3 have quite similar missing%. \n",
    "We could impute some values very precisely by insert the mean of the other values within the triplet, \n",
    "if only one or two values are missing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_filter(dataframe):\n",
    "    \"\"\" Filter out NaN\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    filtered = dataframe.dropna(how='all', axis= 0, subset=None, thresh=None).compute()\n",
    "\n",
    "    return filtered\n",
    "\n",
    "filtered = nan_filter(x_train_df)\n",
    "percent_missing(filtered)\n",
    "\n",
    "#THIS ONE DOES NOT WORK, THE AXIS PARAMETER SEEMS TO HAVE BEEN DEPRECATED. \n",
    "#DROPPING A WITHIN SPECIFYING ROWS OR COLUMNS DID NOT REMOVE ANY OF EITHER\n",
    "\n",
    "#remove columns where all observations are NaN: \n",
    "#print(len(x_train_df.columns))\n",
    "#print(len(x_train_df))\n",
    "#x_train_df_nafiltered = x_train_df.dropna(how='all').compute()\n",
    "#print(len(x_train_df.columns))\n",
    "#print(len(x_train_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, thresh_perc, ncols = None, nrows = None):\n",
    "    \"\"\" Drop columns which have more than 50% missing\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if ncols == None: ncols = len(df.columns)\n",
    "    if nrows == None: nrows = len(df)\n",
    "    \n",
    "    nmissing = list(df.isna().sum().compute())\n",
    "    colstodrop = [a for a in range(ncols) if nmissing[a]/nrows*100 >= thresh_perc]\n",
    "    colstodrop = [df.columns[a] for a in colstodrop]\n",
    "    return df.drop(colstodrop, axis='columns')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete columns (features) with more than N % missing observations from training data set: \n",
    "missing_perc = 50\n",
    "x_train_df = drop_columns(x_train_df, missing_perc, x_train_df_ncols, x_train_df_nrows)\n",
    "\n",
    "diff = x_train_df_ncols - len(x_train_df.columns)\n",
    "if diff > 0: \n",
    "    print('Deleted', diff, 'features because they had more than', missing_perc, 'percent missing observations')\n",
    "    x_train_df_ncols = len(x_train_df.columns)\n",
    "else:\n",
    "    print('No features had more than', missing_perc, 'percent missing observations')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute by inserting value from most closely correlated column: \n",
    "\n",
    "\"\"\"\n",
    "I can't understand why this one doesn't work. The idea was to fill in NaN's from correlated columns.\n",
    "The problem might be in the last line\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#will only correct if the correlation between columns is above this threshold: \n",
    "lowest_allowed_corr = 0.995\n",
    "\n",
    "if 'correlated_columns' in imputation_methods:\n",
    "    \n",
    "    #if correlation matrix does not exist, first try to read it from file, if that does not work calculate it: \n",
    "    if dir().count('corrs') == 0: \n",
    "        try: \n",
    "            corrs = pd.read_csv(workdir + 'features_correlation_matrix.txt')\n",
    "        except:\n",
    "            corrs = x_train_df.corr('pearson')\n",
    "    \n",
    "    #impute for each feature feat:\n",
    "    for feat in x_train_df_colnames:\n",
    "        \n",
    "        #order the feature names according to (absolute value of) correlations to feat:\n",
    "        abscorr = [abs(a) for a in list(corrs[feat])]\n",
    "        order = np.argsort(abscorr)[::-1]\n",
    "        topfeatures = [x_train_df_colnames[a] for a in order]\n",
    "        \n",
    "        #remove features which are not sufficiently correlated to feat:\n",
    "        mapper = dict(zip(x_train_df_colnames, abscorr))\n",
    "        topfeatures = [a for a in topfeatures if mapper[a] >= lowest_allowed_corr]\n",
    "        \n",
    "        #correct using each feature in topfeatures, starting with the most strongly correlated otherfeature:\n",
    "        for otherfeat in [a for a in topfeatures if a != feat]:\n",
    "            print(feat, otherfeat)\n",
    "            x_train_df_trim[feat] = x_train_df_trim[feat].fillna(x_train_df_trim[otherfeat]).compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute by inserting the mean of the column in question, for all columns: \n",
    " \n",
    "if 'col_means' in imputation_methods:\n",
    "    \n",
    "    #calculate mean (note that axis needs to be 0 to get columns, which is weird)\n",
    "    miin = x_train_df.mean(axis = 0).compute() # Fill with mean value\n",
    "    miin2 = y_train_df.min(axis = 0).compute() # Fill with zeros\n",
    "\n",
    "    x_train_df_imean = x_train_df.fillna(dict(miin))\n",
    "    y_train_df_imean = y_train_df.fillna(dict(miin2))\n",
    "\n",
    "    # Transfer back to dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(dataframe):\n",
    "    \"\"\" Drop rows if more than 0 and less than 5% missing \n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    rows_to_drop = list(percent_missing[(percent_missing > 0) & (percent_missing < 5)].index)\n",
    "    data_clean = dataframe.dropna(subset=rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_rows(dataframe,feature):\n",
    "    \"\"\" Impute rows if more than 5% and less than 50% missing\n",
    "    :param data: dataframe, feature\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    value = median[feature]\n",
    "    df_impute[feature] = df[feature].fillna(value)\n",
    "    return df_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of a feature\n",
    "print (x_train_df.count().compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and train\n",
    "\n",
    "# Import libraries\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.metrics import accuracy_score\n",
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df_imean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = x_train_df_imean.is_null().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dask array with chunks. Delete obsolete variables\n",
    "X = x_train_df_imean\n",
    "X = X.to_dask_array(lengths=True)\n",
    "y = y_train_df_imean\n",
    "y = y.to_dask_array(lengths=True)\n",
    "y\n",
    "del x_train_df_imean, x_train_df, y_train_df_imean, y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>  <thead>    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 4.09 GB </td> <td> 129.60 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (3155759, 162) </td> <td> (100000, 162) </td></tr>\n",
       "    <tr><th> Count </th><td> 509 Tasks </td><td> 32 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody></table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"25\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"7\" x2=\"25\" y2=\"7\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"15\" x2=\"25\" y2=\"15\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"26\" x2=\"25\" y2=\"26\" />\n",
       "  <line x1=\"0\" y1=\"30\" x2=\"25\" y2=\"30\" />\n",
       "  <line x1=\"0\" y1=\"34\" x2=\"25\" y2=\"34\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"49\" x2=\"25\" y2=\"49\" />\n",
       "  <line x1=\"0\" y1=\"53\" x2=\"25\" y2=\"53\" />\n",
       "  <line x1=\"0\" y1=\"57\" x2=\"25\" y2=\"57\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"25\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"64\" x2=\"25\" y2=\"64\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"25\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"83\" x2=\"25\" y2=\"83\" />\n",
       "  <line x1=\"0\" y1=\"87\" x2=\"25\" y2=\"87\" />\n",
       "  <line x1=\"0\" y1=\"91\" x2=\"25\" y2=\"91\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"98\" x2=\"25\" y2=\"98\" />\n",
       "  <line x1=\"0\" y1=\"102\" x2=\"25\" y2=\"102\" />\n",
       "  <line x1=\"0\" y1=\"106\" x2=\"25\" y2=\"106\" />\n",
       "  <line x1=\"0\" y1=\"110\" x2=\"25\" y2=\"110\" />\n",
       "  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n",
       "  <line x1=\"0\" y1=\"117\" x2=\"25\" y2=\"117\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 25.412617,0.000000 25.412617,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >162</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">3155759</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<rechunk-merge, shape=(3155759, 162), dtype=float64, chunksize=(100000, 162)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert blocks in dask array x for new chunks\n",
    "X = X.rechunk((100000, 162))\n",
    "y = y.rechunk((100000, 1))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>  <thead>    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 22.72 MB </td> <td> 720.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (2840183, 1) </td> <td> (90000, 1) </td></tr>\n",
       "    <tr><th> Count </th><td> 258 Tasks </td><td> 32 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody></table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"25\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"7\" x2=\"25\" y2=\"7\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"15\" x2=\"25\" y2=\"15\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"26\" x2=\"25\" y2=\"26\" />\n",
       "  <line x1=\"0\" y1=\"30\" x2=\"25\" y2=\"30\" />\n",
       "  <line x1=\"0\" y1=\"34\" x2=\"25\" y2=\"34\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"49\" x2=\"25\" y2=\"49\" />\n",
       "  <line x1=\"0\" y1=\"53\" x2=\"25\" y2=\"53\" />\n",
       "  <line x1=\"0\" y1=\"57\" x2=\"25\" y2=\"57\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"25\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"64\" x2=\"25\" y2=\"64\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"25\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"83\" x2=\"25\" y2=\"83\" />\n",
       "  <line x1=\"0\" y1=\"87\" x2=\"25\" y2=\"87\" />\n",
       "  <line x1=\"0\" y1=\"91\" x2=\"25\" y2=\"91\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"98\" x2=\"25\" y2=\"98\" />\n",
       "  <line x1=\"0\" y1=\"102\" x2=\"25\" y2=\"102\" />\n",
       "  <line x1=\"0\" y1=\"106\" x2=\"25\" y2=\"106\" />\n",
       "  <line x1=\"0\" y1=\"110\" x2=\"25\" y2=\"110\" />\n",
       "  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n",
       "  <line x1=\"0\" y1=\"117\" x2=\"25\" y2=\"117\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 25.412617,0.000000 25.412617,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">2840183</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(2840183, 1), dtype=float64, chunksize=(90000, 1)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "del X, y\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client \n",
    "client = Client() # start a local Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:53143\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>51.54 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:53143' processes=4 cores=8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgenkongsro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/jorgenkongsro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_true = y_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy_score(y_true, y_pred)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "    print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "    print('F1_lr: %.3f' % f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of test data\n",
    "\n",
    "confmat_test = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat_test, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat_test.shape[0]):\n",
    "    for j in range(confmat_test.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat_test[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "clf = ParallelPostFit(LogisticRegressionCV(cv=3), scoring=\"r2\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true = y_train\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC, F1 and confusion matrix for the logistic regression model on the training data\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_true = y_train\n",
    "y_pred = lr.predict(X_train)\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = lr.predict(X_test)\n",
    "#accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of test data\n",
    "\n",
    "confmat_test = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat_test, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat_test.shape[0]):\n",
    "    for j in range(confmat_test.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat_test[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
