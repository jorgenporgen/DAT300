{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compulsory assignment 1 - Using Dask-ml on large data\n",
    "## Group #16\n",
    "### Thomas Moen and JÃ¸rgen Kongsro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.metrics import accuracy_score\n",
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "print (dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Kaggle API and download Kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API\n",
    "# How to setup: https://github.com/Kaggle/kaggle-api\n",
    "# or visit: https://adityashrm21.github.io/Setting-Up-Kaggle/\n",
    "\n",
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle data using Kaggle API\n",
    "#!kaggle competitions download -c dat300-ca1-autumn-2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Kaggle data\n",
    "#!unzip \"dat300-ca1-autumn-2019.zip\" -d \"/tmp/whatever\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List files in directory (adjust for different operating systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for different os\n",
    "\n",
    "if os.name == 'nt':\n",
    "    workdir = 'C://Users//thomoe//Documents//myDAT300//dat300-ca1-autumn-2019//'\n",
    "elif os.name == 'posix':\n",
    "    workdir = '/Users/jorgenkongsro/Downloads/dat300-ca1-autumn-2019/'\n",
    "    \n",
    "os.listdir(workdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(csv_file):\n",
    "    \"\"\" Import data from csv file\n",
    "\n",
    "    :param data: a .csv separated dataset\n",
    "    :return: a pandas data array, df\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df = dd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "x_train_df = import_data(workdir + 'X_train.csv')\n",
    "x_test_df = import_data(workdir + 'X_test.csv')\n",
    "y_train_df = import_data(workdir + 'y_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(dataframe):\n",
    "    \"\"\" Check for percent missing values in dataframe\n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    missing_values = dataframe.isnull().sum()\n",
    "    \n",
    "    with ProgressBar():\n",
    "        percent_missing = ((missing_values / dataframe.index.size) * 100).compute()\n",
    "        \n",
    "    return percent_missing\n",
    "\n",
    "\n",
    "print(percent_missing(y_train_df))\n",
    "\n",
    "\"\"\"\n",
    "note: the results indicate that the features come in \"tripets\", e.g. f1 to f3 have quite similar missing%. \n",
    "We could impute some values very precisely by insert the mean of the other values within the triplet, \n",
    "if only one or two values are missing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose here which imputation methods to use:\n",
    "imputation_methods = ['correlated_columns', 'col_means']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_sampleSubmission.csv', 'X_train.csv', 'y_train.csv', 'X_test.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#impute by inserting the mean of the column in question, for all columns: \n",
    " \n",
    "if 'col_means' in imputation_methods:\n",
    "    \n",
    "    #calculate mean (note that axis needs to be 0 to get columns, which is weird)\n",
    "    miin = x_train_df.mean(axis = 0).compute() # Fill with mean value\n",
    "    miin2 = y_train_df.min(axis = 0).compute() # Fill with zeros\n",
    "\n",
    "    x_train_df_imean = x_train_df.fillna(dict(miin))\n",
    "    y_train_df_imean = y_train_df.fillna(dict(miin2))\n",
    "\n",
    "    # Transfer back to dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute by inserting value from most closely correlated column: \n",
    "\n",
    "\"\"\"\n",
    "I can't understand why this one doesn't work. The idea was to fill in NaN's from correlated columns.\n",
    "The problem might be in the last line\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#will only correct if the correlation between columns is above this threshold: \n",
    "lowest_allowed_corr = 0.995\n",
    "\n",
    "if 'correlated_columns' in imputation_methods:\n",
    "    \n",
    "    #if correlation matrix does not exist, first try to read it from file, if that does not work calculate it: \n",
    "    if dir().count('corrs') == 0: \n",
    "        try: \n",
    "            corrs = pd.read_csv(workdir + 'features_correlation_matrix.txt')\n",
    "        except:\n",
    "            corrs = x_train_df.corr('pearson')\n",
    "    \n",
    "    #impute for each feature feat:\n",
    "    for feat in x_train_df_colnames:\n",
    "        \n",
    "        #order the feature names according to (absolute value of) correlations to feat:\n",
    "        abscorr = [abs(a) for a in list(corrs[feat])]\n",
    "        order = np.argsort(abscorr)[::-1]\n",
    "        topfeatures = [x_train_df_colnames[a] for a in order]\n",
    "        \n",
    "        #remove features which are not sufficiently correlated to feat:\n",
    "        mapper = dict(zip(x_train_df_colnames, abscorr))\n",
    "        topfeatures = [a for a in topfeatures if mapper[a] >= lowest_allowed_corr]\n",
    "        \n",
    "        #correct using each feature in topfeatures, starting with the most strongly correlated otherfeature:\n",
    "        for otherfeat in [a for a in topfeatures if a != feat]:\n",
    "            print(feat, otherfeat)\n",
    "            x_train_df_trim[feat] = x_train_df_trim[feat].fillna(x_train_df_trim[otherfeat]).compute()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(dataframe):\n",
    "    \"\"\" Drop rows if more than 0 and less than 5% missing \n",
    "    :param data: dataframe\n",
    "    :return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    rows_to_drop = list(percent_missing[(percent_missing > 0) & (percent_missing < 5)].index)\n",
    "    data_clean = dataframe.dropna(subset=rows_to_drop)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dask array with chunks. Delete obsolete variables\n",
    "X = x_train_df_imean\n",
    "X = X.to_dask_array(lengths=True)\n",
    "y = y_train_df_imean\n",
    "y = y.to_dask_array(lengths=True)\n",
    "y\n",
    "del x_train_df_imean, x_train_df, y_train_df_imean, y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>  <thead>    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 4.09 GB </td> <td> 129.60 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (3155759, 162) </td> <td> (100000, 162) </td></tr>\n",
       "    <tr><th> Count </th><td> 509 Tasks </td><td> 32 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody></table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"25\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"7\" x2=\"25\" y2=\"7\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"15\" x2=\"25\" y2=\"15\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"26\" x2=\"25\" y2=\"26\" />\n",
       "  <line x1=\"0\" y1=\"30\" x2=\"25\" y2=\"30\" />\n",
       "  <line x1=\"0\" y1=\"34\" x2=\"25\" y2=\"34\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"49\" x2=\"25\" y2=\"49\" />\n",
       "  <line x1=\"0\" y1=\"53\" x2=\"25\" y2=\"53\" />\n",
       "  <line x1=\"0\" y1=\"57\" x2=\"25\" y2=\"57\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"25\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"64\" x2=\"25\" y2=\"64\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"25\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"83\" x2=\"25\" y2=\"83\" />\n",
       "  <line x1=\"0\" y1=\"87\" x2=\"25\" y2=\"87\" />\n",
       "  <line x1=\"0\" y1=\"91\" x2=\"25\" y2=\"91\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"98\" x2=\"25\" y2=\"98\" />\n",
       "  <line x1=\"0\" y1=\"102\" x2=\"25\" y2=\"102\" />\n",
       "  <line x1=\"0\" y1=\"106\" x2=\"25\" y2=\"106\" />\n",
       "  <line x1=\"0\" y1=\"110\" x2=\"25\" y2=\"110\" />\n",
       "  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n",
       "  <line x1=\"0\" y1=\"117\" x2=\"25\" y2=\"117\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 25.412617,0.000000 25.412617,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >162</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">3155759</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<rechunk-merge, shape=(3155759, 162), dtype=float64, chunksize=(100000, 162)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert blocks in dask array x for new chunks\n",
    "X = X.rechunk((100000, 162))\n",
    "y = y.rechunk((100000, 1))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>  <thead>    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 22.72 MB </td> <td> 720.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (2840183, 1) </td> <td> (90000, 1) </td></tr>\n",
       "    <tr><th> Count </th><td> 258 Tasks </td><td> 32 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody></table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"25\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"7\" x2=\"25\" y2=\"7\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"15\" x2=\"25\" y2=\"15\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"26\" x2=\"25\" y2=\"26\" />\n",
       "  <line x1=\"0\" y1=\"30\" x2=\"25\" y2=\"30\" />\n",
       "  <line x1=\"0\" y1=\"34\" x2=\"25\" y2=\"34\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"49\" x2=\"25\" y2=\"49\" />\n",
       "  <line x1=\"0\" y1=\"53\" x2=\"25\" y2=\"53\" />\n",
       "  <line x1=\"0\" y1=\"57\" x2=\"25\" y2=\"57\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"25\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"64\" x2=\"25\" y2=\"64\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"25\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"83\" x2=\"25\" y2=\"83\" />\n",
       "  <line x1=\"0\" y1=\"87\" x2=\"25\" y2=\"87\" />\n",
       "  <line x1=\"0\" y1=\"91\" x2=\"25\" y2=\"91\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"98\" x2=\"25\" y2=\"98\" />\n",
       "  <line x1=\"0\" y1=\"102\" x2=\"25\" y2=\"102\" />\n",
       "  <line x1=\"0\" y1=\"106\" x2=\"25\" y2=\"106\" />\n",
       "  <line x1=\"0\" y1=\"110\" x2=\"25\" y2=\"110\" />\n",
       "  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n",
       "  <line x1=\"0\" y1=\"117\" x2=\"25\" y2=\"117\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 25.412617,0.000000 25.412617,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">2840183</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(2840183, 1), dtype=float64, chunksize=(90000, 1)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "del X, y\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "client = Client()  # Connect to a Dask Cluster\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_true = y_test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "We ran into memory error for the model.predict part of the code\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplement Scalable Algorithms with Dask Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(data, labels)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Takes a lot of time to compute\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostics(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the AUC and F1 metric for the model test data\n",
    "    Args:\n",
    "        y_test (list): Measured target test data\n",
    "        y_pred (list): Predicted target data based on model test X\n",
    "\n",
    "    Returns:\n",
    "        auc (int): area under curve metric\n",
    "        F1 (int): F1 score\n",
    "    \"\"\"\n",
    "accuracy_score(y_true, y_pred)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "AUC = metrics.auc(fpr, tpr)\n",
    "F1 = f1_score(y_true, y_pred)\n",
    "\n",
    "return AUC, F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of test data\n",
    "\n",
    "confmat_test = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat_test, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat_test.shape[0]):\n",
    "    for j in range(confmat_test.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat_test[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgenkongsro/anaconda3/lib/python3.7/site-packages/dask_glm/utils.py:97: RuntimeWarning: overflow encountered in log1p\n",
      "  return np.log1p(A)\n",
      "/Users/jorgenkongsro/anaconda3/lib/python3.7/site-packages/dask_glm/utils.py:97: RuntimeWarning: invalid value encountered in log1p\n",
      "  return np.log1p(A)\n"
     ]
    }
   ],
   "source": [
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "clf = ParallelPostFit(LogisticRegressionCV(cv=3), scoring=\"r2\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true = y_train\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC, F1 and confusion matrix for the logistic regression model on the training data\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_true = y_train\n",
    "y_pred = lr.predict(X_train)\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = lr.predict(X_test)\n",
    "#accuracy_score(y_true, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "print('AUC_lr: %.3f' % metrics.auc(fpr, tpr))\n",
    "print('F1_lr: %.3f' % f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of test data\n",
    "\n",
    "confmat_test = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat_test, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat_test.shape[0]):\n",
    "    for j in range(confmat_test.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat_test[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
